# Unified Test Compose - Backend services for all integration test suites
# Uses environment variables for port isolation to enable parallel test execution
#
# Environment variables (set by integration-hooks.js):
#   CONTAINER_PREFIX - Container name prefix (e.g., test-core, test-lite)
#   MONGO_PORT       - MongoDB external port (default: 37017)
#   KAFKA_PORT       - Kafka external port (default: 19092)
#   CH_HTTP_PORT     - ClickHouse HTTP port (default: 18123)
#   CH_NATIVE_PORT   - ClickHouse native port (default: 19000)
#   KAFKA_CONNECT_PORT - Kafka Connect port (default: 18083)
#   NGINX_PORT       - Nginx proxy port (default: 10080)
#   KAFKA_CLUSTER_ID - Unique Kafka cluster ID per suite
#   USE_LOW_MEMORY   - Set to "true" to mount clickhouse-low-memory.xml
#   WIREDTIGER_CACHE - WiredTiger cache size (default: 0.5)

services:
  # ------------------------------
  # MongoDB 8.0 + Replica Set
  # ------------------------------
  mongodb:
    image: mongo:8.0
    container_name: ${CONTAINER_PREFIX:-test}-mongodb
    command: ["mongod", "--replSet", "rs0", "--bind_ip_all", "--wiredTigerCacheSizeGB", "${WIREDTIGER_CACHE:-0.5}"]
    ports:
      - "${MONGO_PORT:-37017}:27017"
    networks:
      - test-net
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "mongosh", "--eval", "db.adminCommand('ping')"]
      interval: 10s
      timeout: 5s
      retries: 5

  mongodb-init:
    image: mongo:8.0
    container_name: ${CONTAINER_PREFIX:-test}-mongodb-init
    depends_on:
      mongodb:
        condition: service_healthy
    command: >
      sh -c "
      mongosh --host mongodb:27017 --eval
      'try{rs.status()}catch(e){rs.initiate({_id:\"rs0\",members:[{_id:0,host:\"mongodb:27017\"}]})}';
      "
    networks:
      - test-net
    restart: "no"

  # Import test data for plugin tests
  mongodb-data-init:
    image: mongo:8.0
    container_name: ${CONTAINER_PREFIX:-test}-mongodb-data-init
    depends_on:
      mongodb-init:
        condition: service_completed_successfully
    volumes:
      - ../../bin/backup:/backup:ro
    command: >
      sh -c "
        echo 'Importing test data...' &&
        mongoimport --host mongodb:27017 --db countly --collection app_crashgroups58bf06bd6cba850047ac9f19 --file /backup/app_crashgroups58bf06bd6cba850047ac9f19.json --upsert &&
        mongoimport --host mongodb:27017 --db countly --collection apps --file /backup/apps.json --upsert &&
        mongoimport --host mongodb:27017 --db countly --collection members --file /backup/members.json --upsert &&
        echo 'Test data imported successfully!'
      "
    networks:
      - test-net
    restart: "no"

  # ------------------------------
  # Kafka (KRaft Mode, Broker + Controller)
  # ------------------------------
  kafka:
    image: apache/kafka:4.0.0
    container_name: ${CONTAINER_PREFIX:-test}-kafka
    hostname: kafka
    ports:
      - "${KAFKA_PORT:-19092}:9092"
    environment:
      KAFKA_PROCESS_ROLES: "broker,controller"
      KAFKA_NODE_ID: 1
      KAFKA_CONTROLLER_LISTENER_NAMES: "CONTROLLER"
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: "PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT,CONTROLLER:PLAINTEXT"
      KAFKA_LISTENERS: "PLAINTEXT://kafka:29092,PLAINTEXT_HOST://0.0.0.0:9092,CONTROLLER://kafka:29093"
      KAFKA_ADVERTISED_LISTENERS: "PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:${KAFKA_PORT:-19092}"
      KAFKA_CONTROLLER_QUORUM_VOTERS: "1@kafka:29093"
      KAFKA_INTER_BROKER_LISTENER_NAME: "PLAINTEXT"
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_LOG_DIRS: "/tmp/kraft-combined-logs"
      CLUSTER_ID: "${KAFKA_CLUSTER_ID:-MkU3OEVBNTcwNTJENDM2Qk}"
      KAFKA_HEAP_OPTS: "-Xms128m -Xmx512m"
      KAFKA_MESSAGE_MAX_BYTES: "52428800"
      KAFKA_REPLICA_FETCH_MAX_BYTES: "52428800"
      KAFKA_SOCKET_REQUEST_MAX_BYTES: "52428800"
      KAFKA_LOG_FLUSH_INTERVAL_MESSAGES: "1"
      KAFKA_LOG_FLUSH_INTERVAL_MS: "0"
      KAFKA_LOG_SEGMENT_BYTES: "10485760"
      # Consumer group settings for single-node
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "false"
      KAFKA_OFFSETS_TOPIC_NUM_PARTITIONS: "1"
      KAFKA_OFFSETS_TOPIC_MIN_ISR: "1"
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: "0"
    networks:
      - test-net
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "/opt/kafka/bin/kafka-broker-api-versions.sh", "--bootstrap-server", "kafka:29092"]
      interval: 10s
      timeout: 5s
      retries: 8

  # ------------------------------
  # Kafka Connect (CI Image with ClickHouse Connector)
  # ------------------------------
  kafka-connect:
    image: gcr.io/countly-dev-313620/kafka-connect-clickhouse-ci:kafka-4.0.0-ch-1.3.5
    container_name: ${CONTAINER_PREFIX:-test}-kafka-connect
    depends_on:
      kafka:
        condition: service_healthy
      clickhouse:
        condition: service_healthy
      connect-topics-init:
        condition: service_completed_successfully
      offsets-topic-init:
        condition: service_completed_successfully
    ports:
      - "${KAFKA_CONNECT_PORT:-18083}:8083"
    environment:
      KAFKA_HEAP_OPTS: "-Xms128m -Xmx512m"
      CONNECT_BOOTSTRAP_SERVERS: "kafka:29092"
      CONNECT_REST_ADVERTISED_HOST_NAME: "kafka-connect"
      CONNECT_REST_ADVERTISED_PORT: "8083"
      CONNECT_LISTENERS: "http://0.0.0.0:8083"
      CONNECT_GROUP_ID: "${CONTAINER_PREFIX:-test}-connect-cluster"
      CONNECT_CONFIG_STORAGE_TOPIC: "connect-configs"
      CONNECT_OFFSET_STORAGE_TOPIC: "connect-offsets"
      CONNECT_STATUS_STORAGE_TOPIC: "connect-status"
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: "1"
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: "1"
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: "1"
      CONNECT_CONFIG_STORAGE_PARTITIONS: "1"
      CONNECT_OFFSET_STORAGE_PARTITIONS: "25"
      CONNECT_STATUS_STORAGE_PARTITIONS: "5"
      # Group coordination settings (required for KRaft single-node)
      CONNECT_CONNECT_PROTOCOL: "SESSIONED"
      CONNECT_REBALANCE_TIMEOUT_MS: "60000"
      CONNECT_WORKER_SYNC_TIMEOUT_MS: "10000"
      CONNECT_WORKER_UNSYNC_BACKOFF_MS: "10000"
      CONNECT_GROUP_INITIAL_REBALANCE_DELAY_MS: "0"
      CONNECT_KEY_CONVERTER: "org.apache.kafka.connect.storage.StringConverter"
      CONNECT_VALUE_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: "false"
      CONNECT_PLUGIN_PATH: "/opt/kafka/plugins"
      # Fast processing with small batches for tests
      CONNECT_CONSUMER_MAX_POLL_RECORDS: "100"
      CONNECT_CONSUMER_FETCH_MIN_BYTES: "1"
      CONNECT_CONSUMER_FETCH_MAX_WAIT_MS: "10"
      CONNECT_OFFSET_FLUSH_INTERVAL_MS: "10"
      CONNECT_OFFSET_FLUSH_TIMEOUT_MS: "5000"
    networks:
      - test-net
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8083/connectors"]
      interval: 10s
      timeout: 5s
      retries: 15
      start_period: 30s

  # ------------------------------
  # DLQ Topic Setup (one-shot)
  # ------------------------------
  dlq-init:
    image: apache/kafka:4.0.0
    container_name: ${CONTAINER_PREFIX:-test}-dlq-init
    depends_on:
      kafka:
        condition: service_healthy
    command: >
      sh -c "
      echo 'Creating DLQ topic...';
      /opt/kafka/bin/kafka-topics.sh --bootstrap-server kafka:29092 --create
      --topic drill-events-dlq --partitions 1 --replication-factor 1
      --config retention.bytes=104857600 --config retention.ms=86400000
      --if-not-exists;
      echo 'DLQ topic ready';
      "
    networks:
      - test-net
    restart: "no"

  # ------------------------------
  # Kafka Connect Internal Topics Setup (one-shot)
  # Pre-create with RF=1 before Kafka Connect starts
  # ------------------------------
  connect-topics-init:
    image: apache/kafka:4.0.0
    container_name: ${CONTAINER_PREFIX:-test}-connect-topics-init
    depends_on:
      kafka:
        condition: service_healthy
    command: >
      sh -c "
      echo 'Creating Kafka Connect internal topics with RF=1...';
      /opt/kafka/bin/kafka-topics.sh --bootstrap-server kafka:29092 --create
      --topic connect-configs --partitions 1 --replication-factor 1
      --config cleanup.policy=compact
      --if-not-exists;
      /opt/kafka/bin/kafka-topics.sh --bootstrap-server kafka:29092 --create
      --topic connect-offsets --partitions 25 --replication-factor 1
      --config cleanup.policy=compact
      --if-not-exists;
      /opt/kafka/bin/kafka-topics.sh --bootstrap-server kafka:29092 --create
      --topic connect-status --partitions 5 --replication-factor 1
      --config cleanup.policy=compact
      --if-not-exists;
      echo 'Kafka Connect internal topics ready';
      "
    networks:
      - test-net
    restart: "no"

  # ------------------------------
  # Consumer Offsets Topic Setup (one-shot)
  # Pre-create __consumer_offsets with RF=1 for single-node KRaft
  # ------------------------------
  offsets-topic-init:
    image: apache/kafka:4.0.0
    container_name: ${CONTAINER_PREFIX:-test}-offsets-topic-init
    depends_on:
      kafka:
        condition: service_healthy
    command: >
      sh -c "
      echo 'Creating __consumer_offsets topic with RF=1...';
      /opt/kafka/bin/kafka-topics.sh --bootstrap-server kafka:29092 --create
      --topic __consumer_offsets --partitions 1 --replication-factor 1
      --config cleanup.policy=compact
      --config min.insync.replicas=1
      --if-not-exists;
      echo '__consumer_offsets topic ready';
      "
    networks:
      - test-net
    restart: "no"

  # ------------------------------
  # ClickHouse
  # ------------------------------
  clickhouse:
    image: clickhouse/clickhouse-server:25.8
    container_name: ${CONTAINER_PREFIX:-test}-clickhouse
    ports:
      - "${CH_HTTP_PORT:-18123}:8123"
      - "${CH_NATIVE_PORT:-19000}:9000"
    volumes:
      - ./conf/clickhouse-low-memory.xml:/etc/clickhouse-server/config.d/low-memory.xml:ro
    environment:
      CLICKHOUSE_DB: countly_drill
      CLICKHOUSE_USER: default
      CLICKHOUSE_PASSWORD: ""
      CLICKHOUSE_SKIP_USER_SETUP: 1
    networks:
      - test-net
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:8123/ping"]
      interval: 10s
      timeout: 5s
      retries: 10

  # ------------------------------
  # Nginx Reverse Proxy
  # Proxies to Countly services running on host machine (test ports)
  # Note: Kafka connector is initialized by integration-hooks.js after Countly API starts
  # ------------------------------
  nginx:
    image: bitnami/nginx:latest
    container_name: ${CONTAINER_PREFIX:-test}-nginx
    depends_on:
      kafka-connect:
        condition: service_healthy
      dlq-init:
        condition: service_completed_successfully
    ports:
      - "${NGINX_PORT:-10080}:8080"
    volumes:
      - ./conf/nginx.generated.conf:/opt/bitnami/nginx/conf/server_blocks/countly.conf:ro
    networks:
      - test-net
    restart: unless-stopped
    extra_hosts:
      - "host.docker.internal:host-gateway"

networks:
  test-net:
    driver: bridge
